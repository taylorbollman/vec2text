{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "tf7NF9JluRPA"
      },
      "outputs": [],
      "source": [
        "# %pip install tiktoken\n",
        "# %pip install cohere\n",
        "# %pip install evaluate sacrebleu bert_score rouge_score datasets accelerate transformers sentence_transformers openai > /dev/null\n",
        "# %pip install openai\n",
        "# %pip install vec2text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "import torch\n",
        "print(torch.version.cuda)\n",
        "print(torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n",
            "1.3.8\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "print(torch.cuda.is_available())\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "import openai\n",
        "print(openai.__version__)\n",
        "from openai import OpenAI\n",
        "\n",
        "import torch\n",
        "import sklearn\n",
        "import scipy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<openai.OpenAI at 0x7fc097efb910>"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# !echo 'OPENAI_API_KEY=\"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\"' >> .env\n",
        "\n",
        "load_dotenv()\n",
        "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "OpenAI(api_key=openai_api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "!export BNB_CUDA_VERSION=\"\"\n",
        "!echo 'export BNB_CUDA_VERSION=\"\"' >> ~/.bashrc\n",
        "!echo 'export BNB_CUDA_VERSION=\"\"' >> ~/.bash_profile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "import vec2text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "Tj1nA001Ge60",
        "outputId": "b75d3e9e-1911-49c8-c0a6-84de9b98c4c8"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aeeb17810a6244639234981276432b68",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26c483e44d34461b9ce3075ab430fd0d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b5ac3a4b4e08440b98ed105d64c21ec8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4112224703274b1385c5033cd4541236",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fe5576515e7140c0a7ec7542933bc2e0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5d75ca1a830e4192be71b5cdd5c7b159",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "248e96e21a9e4e27be0325362f195c68",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/219M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e7ab6a2567ea4b228c91ea79b2a5249a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading tokenizer_config.json:   0%|          | 0.00/1.92k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "271284ede5ab4e4dabb371bc32b706ec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "48f373c667d54645a706c485c7b3bd6c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4fc7e918473545d18477bd568b2bd121",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (â€¦)cial_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "95618b2f68074bb29beb50d95b3c69df",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d20a5df7dd554e5b9e985d2f3f7a62d7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/root/miniforge3/envs/base_ggl/lib/python3.11/site-packages/transformers/models/t5/tokenization_t5_fast.py:160: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e6ce06744cbe4da4a739721d0246b414",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading config.json:   0%|          | 0.00/4.47k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "23d7a6f8d3f34b358beaf2d0b5f9583e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/1.15G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c7fcfb1b8374dc58b653c209722f165",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "14c7b224d4b24bb18ceb5f6d7742b754",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b85f60ed65584defb03b9a458067a88a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading builder script:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "['Morris is a PhD student at Cornell University in New York City',\n",
              " 'It was the age of incredulity, the age of wisdom, the age of apocalypse, the age of apocalypse, it was the age of faith, the age of best faith, it was the age of foolishness']"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "corrector = vec2text.load_corrector(\"text-embedding-ada-002\")\n",
        "\n",
        "vec2text.invert_strings(\n",
        "    [\n",
        "        \"Jack Morris is a PhD student at Cornell Tech in New York City\",\n",
        "        \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity\"\n",
        "    ],\n",
        "    corrector=corrector\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "itfdiyvQQk1v"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Jack Morris is a PhD student in tech at Cornell University in New York City',\n",
              " 'It was the best time of the epoch, it was the worst time of the epoch, it was the age of wisdom, it was the age of incredulity, it was the age of belief']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vec2text.invert_strings(\n",
        "    [\n",
        "        \"Jack Morris is a PhD student at Cornell Tech in New York City\",\n",
        "        \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity\"\n",
        "    ],\n",
        "    corrector=corrector,\n",
        "    num_steps=20,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Uq6eVgL7GrFB"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Jack Morris is a PhD student at Cornell Tech in New York City',\n",
              " 'It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity']"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "vec2text.invert_strings(\n",
        "    [\n",
        "        \"Jack Morris is a PhD student at Cornell Tech in New York City\",\n",
        "        \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity\"\n",
        "    ],\n",
        "    corrector=corrector,\n",
        "    num_steps=20,\n",
        "    sequence_beam_width=4,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKkxKrXELjiJ",
        "outputId": "b64dfade-0ddf-46ce-ec51-b3ebc04a92af"
      },
      "outputs": [
        {
          "ename": "APIRemovedInV1",
          "evalue": "\n\nYou tried to access openai.Embedding, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAPIRemovedInV1\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[18], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m         outputs\u001b[38;5;241m.\u001b[39mextend([e[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(outputs)\n\u001b[0;32m---> 18\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mget_embeddings_openai\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mJack Morris is a PhD student at Cornell Tech in New York City\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIt was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m     21\u001b[0m \u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m vec2text\u001b[38;5;241m.\u001b[39minvert_embeddings(\n\u001b[1;32m     25\u001b[0m     embeddings\u001b[38;5;241m=\u001b[39membeddings\u001b[38;5;241m.\u001b[39mcuda(),\n\u001b[1;32m     26\u001b[0m     corrector\u001b[38;5;241m=\u001b[39mcorrector\n\u001b[1;32m     27\u001b[0m )\n",
            "Cell \u001b[0;32mIn[18], line 9\u001b[0m, in \u001b[0;36mget_embeddings_openai\u001b[0;34m(text_list, model)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batches):\n\u001b[1;32m      8\u001b[0m     text_list_batch \u001b[38;5;241m=\u001b[39m text_list[batch \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m128\u001b[39m : (batch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m128\u001b[39m]\n\u001b[0;32m----> 9\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mopenai\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEmbedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_list_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfloat\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# override default base64 encoding...\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mextend([e[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m response[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]])\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(outputs)\n",
            "File \u001b[0;32m~/miniforge3/envs/base_ggl/lib/python3.11/site-packages/openai/lib/_old_api.py:39\u001b[0m, in \u001b[0;36mAPIRemovedInV1Proxy.__call__\u001b[0;34m(self, *_args, **_kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39m_args: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m APIRemovedInV1(symbol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_symbol)\n",
            "\u001b[0;31mAPIRemovedInV1\u001b[0m: \n\nYou tried to access openai.Embedding, but this is no longer supported in openai>=1.0.0 - see the README at https://github.com/openai/openai-python for the API.\n\nYou can run `openai migrate` to automatically upgrade your codebase to use the 1.0.0 interface. \n\nAlternatively, you can pin your installation to the old version, e.g. `pip install openai==0.28`\n\nA detailed migration guide is available here: https://github.com/openai/openai-python/discussions/742\n"
          ]
        }
      ],
      "source": [
        "import math\n",
        "import torch\n",
        "\n",
        "def get_embeddings_openai(text_list, model=\"text-embedding-ada-002\") -> torch.Tensor:\n",
        "    batches = math.ceil(len(text_list) / 128)\n",
        "    outputs = []\n",
        "    for batch in range(batches):\n",
        "        text_list_batch = text_list[batch * 128 : (batch + 1) * 128]\n",
        "        response = openai.Embedding.create(\n",
        "            input=text_list_batch,\n",
        "            model=model,\n",
        "            encoding_format=\"float\",  # override default base64 encoding...\n",
        "        )\n",
        "        outputs.extend([e[\"embedding\"] for e in response[\"data\"]])\n",
        "    return torch.tensor(outputs)\n",
        "\n",
        "\n",
        "embeddings = get_embeddings_openai([\n",
        "      \"Jack Morris is a PhD student at Cornell Tech in New York City\",\n",
        "      \"It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity\"\n",
        "])\n",
        "\n",
        "\n",
        "vec2text.invert_embeddings(\n",
        "    embeddings=embeddings.cuda(),\n",
        "    corrector=corrector\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xWzzoiLeI_Fw",
        "outputId": "7e35a1c3-004c-4491-f2f6-0496fb626d18"
      },
      "outputs": [],
      "source": [
        "vec2text.invert_embeddings(\n",
        "    embeddings=embeddings.mean(dim=0, keepdim=True).cuda(),\n",
        "    corrector=corrector\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dg_MkQgJB7q",
        "outputId": "21b87255-f8a4-4d1b-8b0a-836678fe0167"
      },
      "outputs": [],
      "source": [
        "embeddings.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ST9WqrK3OK5P",
        "outputId": "8328e07d-2c77-4f2e-d9e2-16067478046e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "for alpha in np.arange(0.0, 1.1, 0.1):\n",
        "  mixed_embedding = torch.lerp(input=embeddings[0], end=embeddings[1], weight=alpha)\n",
        "  text = vec2text.invert_embeddings(\n",
        "      embeddings=mixed_embedding[None].cuda(),\n",
        "      corrector=corrector,\n",
        "      # num_steps=20,\n",
        "      # sequence_beam_width=4,\n",
        "  )[0]\n",
        "  print(f'alpha={alpha:.1f}\\t', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ox9gK3wwJDvM",
        "outputId": "dafa0a10-10e4-4072-cf85-b3051ad61bd8"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "for alpha in np.arange(0.0, 1.0, 0.1):\n",
        "  mixed_embedding = torch.lerp(input=embeddings[0], end=embeddings[1], weight=alpha)\n",
        "  text = vec2text.invert_embeddings(\n",
        "      embeddings=mixed_embedding[None].cuda(),\n",
        "      corrector=corrector,\n",
        "      num_steps=20,\n",
        "      sequence_beam_width=4,\n",
        "  )[0]\n",
        "  print(f'alpha={alpha:.1f}\\t', text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
